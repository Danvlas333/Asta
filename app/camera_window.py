# app/camera_window.py - –û–∫–Ω–æ –∫–∞–º–µ—Ä—ã —Å –¥–µ—Ç–µ–∫—Ü–∏–µ–π –ª–∏—Ü —á–µ—Ä–µ–∑ OpenCV DNN (–±–µ–∑ PyTorch)
import cv2
import numpy as np
from datetime import datetime
import os
from pathlib import Path
from PyQt5.QtWidgets import (
    QWidget, QLabel, QVBoxLayout, QHBoxLayout,
    QPushButton, QMessageBox, QCheckBox, 
    QSpinBox, QFormLayout, QGroupBox, QComboBox,
    QProgressBar
)
from PyQt5.QtCore import Qt, QTimer, QThread, pyqtSignal
from PyQt5.QtGui import QImage, QPixmap

from app.config import STYLES
from app.network import ConnectionManager
from app.workers import CameraWorker

class FaceDetectorDNN:
    """–î–µ—Ç–µ–∫—Ç–æ—Ä –ª–∏—Ü –Ω–∞ –æ—Å–Ω–æ–≤–µ OpenCV DNN —Å YOLO –∏–ª–∏ SSD –º–æ–¥–µ–ª—è–º–∏"""
    
    def __init__(self, model_type='yolov3_face', conf_threshold=0.5):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞
        
        Args:
            model_type: —Ç–∏–ø –º–æ–¥–µ–ª–∏
            conf_threshold: –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        """
        self.conf_threshold = conf_threshold
        self.nms_threshold = 0.4
        self.model_type = model_type
        self.net = None
        self.classes = ['face']
        
        # –†–∞–∑–º–µ—Ä—ã –≤—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        self.model_configs = {
            'yolov3_face': {
                'config': 'models/yolov3-face.cfg',
                'weights': 'models/yolov3-face.weights',
                'input_size': (416, 416),
                'scale': 1/255.0,
                'swap_rb': True
            },
            'tiny_yolo': {
                'config': 'models/yolov3-tiny.cfg',
                'weights': 'models/yolov3-tiny.weights',
                'input_size': (416, 416),
                'scale': 1/255.0,
                'swap_rb': True
            },
            'opencv_face_detector': {
                'config': 'models/opencv_face_detector.pbtxt',
                'weights': 'models/opencv_face_detector_uint8.pb',
                'input_size': (300, 300),
                'scale': 1.0,
                'swap_rb': False,
                'mean': (104.0, 177.0, 123.0)
            },
            'haarcascade': {
                'cascade': 'models/haarcascade_frontalface_default.xml',
                'input_size': None,
                'scale': None,
                'swap_rb': None
            }
        }
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        self.load_model(model_type)
        
    def load_model(self, model_type):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            if model_type == 'haarcascade':
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞—Å–∫–∞–¥—ã Haar (—Å–∞–º—ã–π –ª–µ–≥–∫–∏–π –≤–∞—Ä–∏–∞–Ω—Ç)
                model_path = self.model_configs[model_type]['cascade']
                if os.path.exists(model_path):
                    self.detector = cv2.CascadeClassifier(model_path)
                    self.net = None  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º DNN –¥–ª—è –∫–∞—Å–∫–∞–¥–æ–≤
                    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω Haar cascade: {model_path}")
                else:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –∫–∞—Å–∫–∞–¥ OpenCV
                    self.detector = cv2.CascadeClassifier(
                        cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
                    )
                    print("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π Haar cascade")
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º DNN –º–æ–¥–µ–ª—å
                config = self.model_configs[model_type]
                config_path = config['config']
                weights_path = config['weights']
                
                if os.path.exists(config_path) and os.path.exists(weights_path):
                    self.net = cv2.dnn.readNetFromDarknet(config_path, weights_path)
                    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å {model_type}: {config_path}")
                else:
                    # –ï—Å–ª–∏ —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞—Å–∫–∞–¥
                    print(f"–§–∞–π–ª—ã –º–æ–¥–µ–ª–∏ {model_type} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Haar cascade")
                    self.detector = cv2.CascadeClassifier(
                        cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
                    )
                    self.net = None
                    
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_type}: {e}")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞—Å–∫–∞–¥ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            self.detector = cv2.CascadeClassifier(
                cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
            )
            self.net = None
    
    def detect_faces_dnn(self, frame):
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ª–∏—Ü —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º DNN"""
        if self.net is None:
            return frame, []
            
        config = self.model_configs[self.model_type]
        input_size = config['input_size']
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è DNN
        blob = cv2.dnn.blobFromImage(
            frame, 
            scalefactor=config['scale'],
            size=input_size,
            mean=config.get('mean', (0, 0, 0)),
            swapRB=config['swap_rb'],
            crop=False
        )
        
        # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ —Å–µ—Ç—å
        self.net.setInput(blob)
        outputs = self.net.forward(self.get_output_layers())
        
        # –†–∞–∑–º–µ—Ä—ã –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        height, width = frame.shape[:2]
        faces = []
        boxes = []
        confidences = []
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        for output in outputs:
            for detection in output:
                scores = detection[5:]
                class_id = np.argmax(scores)
                confidence = scores[class_id]
                
                if confidence > self.conf_threshold and class_id == 0:  # class_id 0 –¥–ª—è –ª–∏—Ü–∞
                    # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—é—â–µ–≥–æ –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫–∞
                    center_x = int(detection[0] * width)
                    center_y = int(detection[1] * height)
                    w = int(detection[2] * width)
                    h = int(detection[3] * height)
                    
                    # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –ª–µ–≤–æ–≥–æ –≤–µ—Ä—Ö–Ω–µ–≥–æ —É–≥–ª–∞
                    x = int(center_x - w / 2)
                    y = int(center_y - h / 2)
                    
                    boxes.append([x, y, w, h])
                    confidences.append(float(confidence))
        
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ Non-Maximum Suppression
        indices = cv2.dnn.NMSBoxes(boxes, confidences, self.conf_threshold, self.nms_threshold)
        
        annotated_frame = frame.copy()
        
        if len(indices) > 0:
            for i in indices.flatten():
                x, y, w, h = boxes[i]
                confidence = confidences[i]
                
                # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ª–∏—Ü–∞ –≤ —Å–ø–∏—Å–æ–∫
                faces.append({
                    'bbox': (x, y, x + w, y + h),
                    'confidence': confidence,
                    'class': 0
                })
                
                # –û—Ç—Ä–∏—Å–æ–≤–∫–∞ —Ä–∞–º–∫–∏
                color = (0, 255, 0)
                cv2.rectangle(annotated_frame, (x, y), (x + w, y + h), color, 2)
                
                # –ü–æ–¥–ø–∏—Å—å —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
                label = f"Face: {confidence:.2f}"
                cv2.putText(annotated_frame, label, (x, y - 10),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
        
        return annotated_frame, faces
    
    def detect_faces_haar(self, frame):
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ª–∏—Ü —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–∞—Å–∫–∞–¥–æ–≤ Haar"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ª–∏—Ü
        faces_rect = self.detector.detectMultiScale(
            gray,
            scaleFactor=1.1,
            minNeighbors=5,
            minSize=(30, 30),
            flags=cv2.CASCADE_SCALE_IMAGE
        )
        
        annotated_frame = frame.copy()
        faces = []
        
        for (x, y, w, h) in faces_rect:
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ª–∏—Ü–∞ –≤ —Å–ø–∏—Å–æ–∫ (–±–µ–∑ confidence –¥–ª—è Haar)
            faces.append({
                'bbox': (x, y, x + w, y + h),
                'confidence': 1.0,  # Haar –Ω–µ –¥–∞–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                'class': 0
            })
            
            # –û—Ç—Ä–∏—Å–æ–≤–∫–∞ —Ä–∞–º–∫–∏
            color = (0, 255, 0)
            cv2.rectangle(annotated_frame, (x, y), (x + w, y + h), color, 2)
            
            # –ü–æ–¥–ø–∏—Å—å
            label = "Face"
            cv2.putText(annotated_frame, label, (x, y - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
        
        return annotated_frame, faces
    
    def get_output_layers(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—ã—Ö–æ–¥–Ω—ã—Ö —Å–ª–æ–µ–≤ —Å–µ—Ç–∏"""
        layer_names = self.net.getLayerNames()
        try:
            output_layers = [layer_names[i - 1] for i in self.net.getUnconnectedOutLayers()]
        except:
            output_layers = [layer_names[i[0] - 1] for i in self.net.getUnconnectedOutLayers()]
        return output_layers
    
    def detect_faces(self, frame):
        """
        –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ª–∏—Ü –≤ –∫–∞–¥—Ä–µ
        
        Args:
            frame: –≤—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (BGR)
            
        Returns:
            tuple: (frame —Å –æ—Ç—Ä–∏—Å–æ–≤–∞–Ω–Ω—ã–º–∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—é—â–∏–º–∏ —Ä–∞–º–∫–∞–º–∏, —Å–ø–∏—Å–æ–∫ –ª–∏—Ü)
        """
        try:
            if self.net is not None:
                return self.detect_faces_dnn(frame)
            else:
                return self.detect_faces_haar(frame)
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏: {e}")
            return frame, []
    
    def set_confidence_threshold(self, conf_threshold):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ—Ä–æ–≥–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
        self.conf_threshold = conf_threshold
        
    def set_model(self, model_type):
        """–°–º–µ–Ω–∞ –º–æ–¥–µ–ª–∏"""
        self.model_type = model_type
        self.load_model(model_type)

class FaceDetectionWorker(QThread):
    """–†–∞–±–æ—á–∏–π –ø–æ—Ç–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏ –ª–∏—Ü"""
    
    frame_processed = pyqtSignal(np.ndarray, list)  # –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∫–∞–¥—Ä –∏ —Å–ø–∏—Å–æ–∫ –ª–∏—Ü
    detection_info = pyqtSignal(dict)  # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–µ—Ç–µ–∫—Ü–∏–∏
    
    def __init__(self):
        super().__init__()
        self.detector = None
        self.current_frame = None
        self.running = False
        self.enabled = True
        self.confidence = 0.5
        
    def set_detector(self, detector):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞"""
        self.detector = detector
        
    def process_frame(self, frame):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–¥—Ä–∞"""
        self.current_frame = frame
        
        if not self.running:
            self.start()
            
    def run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        self.running = True
        
        while self.running and self.current_frame is not None:
            if self.detector and self.enabled:
                # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ª–∏—Ü
                processed_frame, faces = self.detector.detect_faces(self.current_frame)
                
                # –û—Ç–ø—Ä–∞–≤–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ –∫–∞–¥—Ä–∞
                self.frame_processed.emit(processed_frame, faces)
                
                # –û—Ç–ø—Ä–∞–≤–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                stats = {
                    'faces_detected': len(faces),
                    'timestamp': datetime.now().strftime("%H:%M:%S"),
                    'model_type': self.detector.model_type
                }
                if faces:
                    confidences = [face['confidence'] for face in faces]
                    stats['avg_confidence'] = np.mean(confidences)
                    stats['max_confidence'] = np.max(confidences)
                    
                self.detection_info.emit(stats)
                
            else:
                # –ï—Å–ª–∏ –¥–µ—Ç–µ–∫—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∫–∞–¥—Ä
                self.frame_processed.emit(self.current_frame, [])
                
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏
            self.msleep(10)
            
    def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ—Ç–æ–∫–∞"""
        self.running = False
        self.wait()

class CameraViewer(QWidget):
    """–û–∫–Ω–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –∫–∞–º–µ—Ä—ã —Å –¥–µ—Ç–µ–∫—Ü–∏–µ–π –ª–∏—Ü"""
    
    def __init__(self):
        super().__init__()
        self.setWindowTitle("üì∑ –í–∏–¥–µ–æ —Å Raspberry Pi - –î–µ—Ç–µ–∫—Ü–∏—è –ª–∏—Ü")
        self.resize(900, 700)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞
        self.detector = None
        self.face_detection_worker = None
        self.current_faces = []
        self.detection_stats = {}
        
        self._init_ui()
        self._init_timers()
        self._init_detector()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –∫–∞–º–µ—Ä—ã
        QTimer.singleShot(100, self.check_camera_status)
        
    def _init_ui(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞"""
        main_layout = QVBoxLayout()
        
        # –ü–∞–Ω–µ–ª—å —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        main_layout.addLayout(self._create_control_panel())
        
        # –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
        content_layout = QHBoxLayout()
        
        # –õ–µ–≤–∞—è –ø–∞–Ω–µ–ª—å - –≤–∏–¥–µ–æ
        left_panel = QVBoxLayout()
        left_panel.addWidget(self._create_video_display())
        left_panel.addLayout(self._create_info_panel())
        content_layout.addLayout(left_panel)
        
        # –ü—Ä–∞–≤–∞—è –ø–∞–Ω–µ–ª—å - –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏
        content_layout.addWidget(self._create_detection_panel())
        
        main_layout.addLayout(content_layout)
        
        # –ü–∞–Ω–µ–ª—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        main_layout.addLayout(self._create_stats_panel())
        
        self.setLayout(main_layout)
        
    def _create_control_panel(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–Ω–µ–ª–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è"""
        layout = QHBoxLayout()
        
        self.status_label = QLabel("–°—Ç–∞—Ç—É—Å: –ù–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–æ")
        self.status_label.setStyleSheet("font-weight: bold;")
        layout.addWidget(self.status_label)
        
        layout.addStretch()
        
        self.start_btn = QPushButton("‚ñ∂Ô∏è –ó–∞–ø—É—Å—Ç–∏—Ç—å –∫–∞–º–µ—Ä—É")
        self.start_btn.clicked.connect(self.start_camera)
        self.start_btn.setStyleSheet(STYLES["success_button"])
        layout.addWidget(self.start_btn)
        
        self.stop_btn = QPushButton("‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å")
        self.stop_btn.clicked.connect(self.stop_camera)
        self.stop_btn.setEnabled(False)
        self.stop_btn.setStyleSheet(STYLES["danger_button"])
        layout.addWidget(self.stop_btn)
        
        self.snapshot_btn = QPushButton("üì∏ –°–Ω–∏–º–æ–∫")
        self.snapshot_btn.clicked.connect(self.take_snapshot)
        self.snapshot_btn.setEnabled(False)
        self.snapshot_btn.setStyleSheet(STYLES["info_button"])
        layout.addWidget(self.snapshot_btn)
        
        return layout
        
    def _create_video_display(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ–∫–Ω–∞ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤–∏–¥–µ–æ"""
        self.video_label = QLabel()
        self.video_label.setAlignment(Qt.AlignCenter)
        self.video_label.setMinimumSize(640, 480)
        self.video_label.setStyleSheet("""
            QLabel {
                background-color: #000;
                border: 2px solid #6c757d;
                border-radius: 5px;
            }
        """)
        self.video_label.setText("–ù–µ—Ç —Å–∏–≥–Ω–∞–ª–∞")
        
        return self.video_label
        
    def _create_detection_panel(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–Ω–µ–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –¥–µ—Ç–µ–∫—Ü–∏–∏"""
        group_box = QGroupBox("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏ –ª–∏—Ü")
        group_box.setMaximumWidth(250)
        layout = QFormLayout()
        
        # –í–∫–ª—é—á–µ–Ω–∏–µ/–æ—Ç–∫–ª—é—á–µ–Ω–∏–µ –¥–µ—Ç–µ–∫—Ü–∏–∏
        self.detection_enabled = QCheckBox("–í–∫–ª—é—á–∏—Ç—å –¥–µ—Ç–µ–∫—Ü–∏—é –ª–∏—Ü")
        self.detection_enabled.setChecked(True)
        self.detection_enabled.stateChanged.connect(self.toggle_detection)
        layout.addRow(self.detection_enabled)
        
        # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
        model_layout = QHBoxLayout()
        self.model_combo = QComboBox()
        self.model_combo.addItems(["haarcascade", "opencv_face_detector", "tiny_yolo", "yolov3_face"])
        self.model_combo.currentTextChanged.connect(self.change_model)
        model_layout.addWidget(QLabel("–ú–æ–¥–µ–ª—å:"))
        model_layout.addWidget(self.model_combo)
        layout.addRow(model_layout)
        
        # –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        self.confidence_spin = QSpinBox()
        self.confidence_spin.setRange(1, 99)
        self.confidence_spin.setValue(50)
        self.confidence_spin.setSuffix("%")
        self.confidence_spin.valueChanged.connect(self.update_confidence)
        layout.addRow("–ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏:", self.confidence_spin)
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
        self.model_info_label = QLabel("–ú–æ–¥–µ–ª—å: Haar Cascade (–≤—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è)\n–°—Ç–∞—Ç—É—Å: –ì–æ—Ç–æ–≤–æ ‚úì")
        self.model_info_label.setWordWrap(True)
        layout.addRow(self.model_info_label)
        
        group_box.setLayout(layout)
        return group_box
        
    def _create_info_panel(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π –ø–∞–Ω–µ–ª–∏"""
        layout = QHBoxLayout()
        
        self.fps_label = QLabel("FPS: --")
        self.resolution_label = QLabel("–†–∞–∑—Ä–µ—à–µ–Ω–∏–µ: --")
        self.timestamp_label = QLabel("–í—Ä–µ–º—è: --")
        
        layout.addWidget(self.fps_label)
        layout.addWidget(self.resolution_label)
        layout.addWidget(self.timestamp_label)
        layout.addStretch()
        
        return layout
        
    def _create_stats_panel(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–Ω–µ–ª–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        layout = QHBoxLayout()
        
        self.faces_count_label = QLabel("–õ–∏—Ü –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ: 0")
        self.faces_count_label.setStyleSheet("font-weight: bold; color: #28a745;")
        
        self.avg_confidence_label = QLabel("–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: --")
        self.max_confidence_label = QLabel("–ú–∞–∫—Å. —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: --")
        
        self.model_type_label = QLabel("–ú–æ–¥–µ–ª—å: Haar Cascade")
        
        layout.addWidget(self.faces_count_label)
        layout.addWidget(self.avg_confidence_label)
        layout.addWidget(self.max_confidence_label)
        layout.addWidget(self.model_type_label)
        layout.addStretch()
        
        return layout
        
    def _init_timers(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–∞–π–º–µ—Ä–æ–≤"""
        # –¢–∞–π–º–µ—Ä –¥–ª—è –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
        self.status_timer = QTimer()
        self.status_timer.timeout.connect(self.check_camera_status)
        self.status_timer.start(5000)
        
        # –¢–∞–π–º–µ—Ä –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ FPS
        self.fps_timer = QTimer()
        self.fps_timer.timeout.connect(self.update_fps)
        self.fps_timer.start(1000)
        
        # –°—á–µ—Ç—á–∏–∫ –∫–∞–¥—Ä–æ–≤ –¥–ª—è FPS
        self.frame_count = 0
        self.fps = 0
        
        # –¢–∞–π–º–µ—Ä –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        self.stats_timer = QTimer()
        self.stats_timer.timeout.connect(self.update_stats_display)
        self.stats_timer.start(100)
        
    def _init_detector(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞"""
        try:
            # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –º–æ–¥–µ–ª–µ–π –µ—Å–ª–∏ –µ–µ –Ω–µ—Ç
            os.makedirs('models', exist_ok=True)
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ —Å Haar cascade (—Å–∞–º—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç)
            self.detector = FaceDetectorDNN('haarcascade', 0.5)
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—á–µ–≥–æ –ø–æ—Ç–æ–∫–∞
            self.face_detection_worker = FaceDetectionWorker()
            self.face_detection_worker.set_detector(self.detector)
            self.face_detection_worker.frame_processed.connect(self.display_processed_frame)
            self.face_detection_worker.detection_info.connect(self.update_detection_stats)
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞: {e}")
            self.model_info_label.setText(f"–û—à–∏–±–∫–∞: {str(e)}")
            self.detection_enabled.setEnabled(False)
        
    def toggle_detection(self, state):
        """–í–∫–ª—é—á–µ–Ω–∏–µ/–æ—Ç–∫–ª—é—á–µ–Ω–∏–µ –¥–µ—Ç–µ–∫—Ü–∏–∏"""
        if self.face_detection_worker:
            self.face_detection_worker.enabled = (state == Qt.Checked)
            
    def update_confidence(self, value):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
        confidence = value / 100.0
        if self.detector:
            self.detector.set_confidence_threshold(confidence)
        if self.face_detection_worker:
            self.face_detection_worker.confidence = confidence
            
    def change_model(self, model_name):
        """–°–º–µ–Ω–∞ –º–æ–¥–µ–ª–∏"""
        try:
            if self.detector:
                self.detector.set_model(model_name)
                
            self.model_info_label.setText(f"–ú–æ–¥–µ–ª—å: {model_name}\n–°—Ç–∞—Ç—É—Å: –ó–∞–≥—Ä—É–∂–µ–Ω–∞ ‚úì")
            self.model_type_label.setText(f"–ú–æ–¥–µ–ª—å: {model_name}")
            
        except Exception as e:
            self.model_info_label.setText(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {str(e)}")
            
    def check_camera_status(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –∫–∞–º–µ—Ä—ã"""
        status = ConnectionManager.get_server_status()
        if status:
            camera_running = status.get("camera_running", False)
            self._update_camera_status(camera_running)
        else:
            self._update_camera_status(False)
            
    def _update_camera_status(self, is_running: bool):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∫–∞–º–µ—Ä—ã"""
        if is_running:
            self.status_label.setText("–°—Ç–∞—Ç—É—Å: –ö–∞–º–µ—Ä–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç")
            self.status_label.setStyleSheet("font-weight: bold; color: green;")
            self.start_btn.setEnabled(False)
            self.stop_btn.setEnabled(True)
            self.snapshot_btn.setEnabled(True)
            self._start_stream()
        else:
            self.status_label.setText("–°—Ç–∞—Ç—É—Å: –ö–∞–º–µ—Ä–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
            self.status_label.setStyleSheet("font-weight: bold; color: orange;")
            self.start_btn.setEnabled(True)
            self.stop_btn.setEnabled(False)
            self.snapshot_btn.setEnabled(False)
            self._stop_stream()
            
    def _start_stream(self):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª—É—á–µ–Ω–∏—è –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–∞"""
        if hasattr(self, 'camera_worker') and self.camera_worker.isRunning():
            return
            
        self.camera_worker = CameraWorker("stream")
        self.camera_worker.frame_ready.connect(self.process_frame_for_detection)
        self.camera_worker.error_occurred.connect(self.on_stream_error)
        self.camera_worker.start()
        
    def _stop_stream(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–∞"""
        if hasattr(self, 'camera_worker') and self.camera_worker.isRunning():
            self.camera_worker.stop()
            
        if self.face_detection_worker:
            self.face_detection_worker.stop()
            
        self.video_label.setText("–ù–µ—Ç —Å–∏–≥–Ω–∞–ª–∞")
        
    def process_frame_for_detection(self, frame_data):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–¥—Ä–∞ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏"""
        self.frame_count += 1
        self.current_frame_data = frame_data  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è —Å–Ω–∏–º–∫–æ–≤
        
        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ JPEG
        nparr = np.frombuffer(frame_data, np.uint8)
        frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        if frame is not None:
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–∏
            height, width = frame.shape[:2]
            self.resolution_label.setText(f"–†–∞–∑—Ä–µ—à–µ–Ω–∏–µ: {width}x{height}")
            self.timestamp_label.setText(f"–í—Ä–µ–º—è: {datetime.now().strftime('%H:%M:%S')}")
            
            # –û—Ç–ø—Ä–∞–≤–∫–∞ –∫–∞–¥—Ä–∞ –≤ —Ä–∞–±–æ—á–∏–π –ø–æ—Ç–æ–∫ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏
            if self.face_detection_worker and self.detection_enabled.isChecked():
                self.face_detection_worker.process_frame(frame.copy())
            else:
                # –ï—Å–ª–∏ –¥–µ—Ç–µ–∫—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞, –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∫–∞–¥—Ä
                self.display_original_frame(frame)
                
    def display_processed_frame(self, processed_frame, faces):
        """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ –∫–∞–¥—Ä–∞ —Å –¥–µ—Ç–µ–∫—Ü–∏–µ–π"""
        self.current_faces = faces
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è BGR –≤ RGB
        rgb_image = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ QImage
        h, w, ch = rgb_image.shape
        bytes_per_line = ch * w
        qt_image = QImage(rgb_image.data, w, h, bytes_per_line, QImage.Format_RGB888)
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        pixmap = QPixmap.fromImage(qt_image)
        pixmap = pixmap.scaled(self.video_label.size(), Qt.KeepAspectRatio, Qt.SmoothTransformation)
        
        self.video_label.setPixmap(pixmap)
        
    def display_original_frame(self, frame):
        """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∫–∞–¥—Ä–∞ –±–µ–∑ –¥–µ—Ç–µ–∫—Ü–∏–∏"""
        rgb_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        h, w, ch = rgb_image.shape
        bytes_per_line = ch * w
        qt_image = QImage(rgb_image.data, w, h, bytes_per_line, QImage.Format_RGB888)
        pixmap = QPixmap.fromImage(qt_image)
        pixmap = pixmap.scaled(self.video_label.size(), Qt.KeepAspectRatio, Qt.SmoothTransformation)
        self.video_label.setPixmap(pixmap)
        
    def update_fps(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ FPS"""
        self.fps = self.frame_count
        self.frame_count = 0
        self.fps_label.setText(f"FPS: {self.fps}")
        
    def update_detection_stats(self, stats):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏"""
        self.detection_stats = stats
        
    def update_stats_display(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        if hasattr(self, 'detection_stats'):
            faces_count = self.detection_stats.get('faces_detected', 0)
            self.faces_count_label.setText(f"–õ–∏—Ü –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ: {faces_count}")
            
            if faces_count > 0:
                avg_conf = self.detection_stats.get('avg_confidence', 0)
                max_conf = self.detection_stats.get('max_confidence', 0)
                model_type = self.detection_stats.get('model_type', 'Unknown')
                
                self.avg_confidence_label.setText(f"–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {avg_conf:.2f}")
                self.max_confidence_label.setText(f"–ú–∞–∫—Å. —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {max_conf:.2f}")
                self.model_type_label.setText(f"–ú–æ–¥–µ–ª—å: {model_type}")
            else:
                self.avg_confidence_label.setText("–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: --")
                self.max_confidence_label.setText("–ú–∞–∫—Å. —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: --")
                
    def on_stream_error(self, error_msg):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–∫–∏ –ø–æ—Ç–æ–∫–∞"""
        self.status_label.setText(f"–°—Ç–∞—Ç—É—Å: {error_msg}")
        self.status_label.setStyleSheet("font-weight: bold; color: red;")
        self.video_label.setText(f"–û—à–∏–±–∫–∞: {error_msg}")
        
    def start_camera(self):
        """–ó–∞–ø—É—Å–∫ –∫–∞–º–µ—Ä—ã –Ω–∞ Raspberry Pi"""
        result = ConnectionManager.start_camera()
        if result["success"]:
            self.status_label.setText("–°—Ç–∞—Ç—É—Å: –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è...")
            self.start_btn.setEnabled(False)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ —á–µ—Ä–µ–∑ 2 —Å–µ–∫—É–Ω–¥—ã
            QTimer.singleShot(2000, self.check_camera_status)
        else:
            QMessageBox.warning(
                self, 
                "–û—à–∏–±–∫–∞", 
                f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å –∫–∞–º–µ—Ä—É: {result.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}"
            )
            
    def stop_camera(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –∫–∞–º–µ—Ä—ã –Ω–∞ Raspberry Pi"""
        result = ConnectionManager.stop_camera()
        if result["success"]:
            self.status_label.setText("–°—Ç–∞—Ç—É—Å: –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è...")
            self.stop_btn.setEnabled(False)
            
            # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ—Ç–æ–∫–∞
            self._stop_stream()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ —á–µ—Ä–µ–∑ 2 —Å–µ–∫—É–Ω–¥—ã
            QTimer.singleShot(2000, self.check_camera_status)
        else:
            QMessageBox.warning(
                self,
                "–û—à–∏–±–∫–∞",
                f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∫–∞–º–µ—Ä—É: {result.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}"
            )
            
    def take_snapshot(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–Ω–∏–º–∫–∞"""
        if not hasattr(self, 'current_frame_data'):
            QMessageBox.warning(self, "–û—à–∏–±–∫–∞", "–ù–µ—Ç —Ç–µ–∫—É—â–µ–≥–æ –∫–∞–¥—Ä–∞ –¥–ª—è —Å–Ω–∏–º–∫–∞")
            return
            
        try:
            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ JPEG
            nparr = np.frombuffer(self.current_frame_data, np.uint8)
            frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
            
            if frame is not None:
                # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –¥–µ—Ç–µ–∫—Ü–∏–∏ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
                if self.detector and self.detection_enabled.isChecked():
                    frame, faces = self.detector.detect_faces(frame)
                    faces_count = len(faces)
                else:
                    faces_count = 0
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"snapshot_{timestamp}.jpg"
                cv2.imwrite(filename, frame)
                
                QMessageBox.information(
                    self,
                    "–°–Ω–∏–º–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω",
                    f"–°–Ω–∏–º–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫: {filename}\n–õ–∏—Ü –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ: {faces_count}"
                )
                
        except Exception as e:
            QMessageBox.warning(
                self,
                "–û—à–∏–±–∫–∞",
                f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–Ω–∏–º–æ–∫: {str(e)}"
            )
            
    def closeEvent(self, event):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–∫—Ä—ã—Ç–∏—è –æ–∫–Ω–∞"""
        self._stop_stream()
        
        if hasattr(self, 'status_timer'):
            self.status_timer.stop()
        if hasattr(self, 'fps_timer'):
            self.fps_timer.stop()
        if hasattr(self, 'stats_timer'):
            self.stats_timer.stop()
        if self.face_detection_worker:
            self.face_detection_worker.stop()
            
        event.accept()